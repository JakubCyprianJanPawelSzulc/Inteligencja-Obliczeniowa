{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praca domowa nr 2 Jakub Szulc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crimes in US Communities Dataset - jest to zestaw danych o przestępstwach w Stanach Zjednoczonych w roku 2018 w podziale na społeczności z podaniem ich statystyk takich jak populacja, wiek, rasa, dochód, a ostatnie kolumny to rodzaje przestępstw i ogólne wskaźniki przestępczości. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej wycinek z danych:\n",
    "\n",
    "communityName,state,countyCode,communityCode,population,householdsize,racepctblack,racePctWhite,racePctAsian,racePctHisp,agePct12t21,agePct12t29,agePct16t24,agePct65up,numbUrban,pctUrban,medIncome,pctWWage,pctWFarmSelf,pctWInvInc,pctWSocSec,pctWPubAsst,pctWRetire,medFamInc,perCapInc,whitePerCap,blackPerCap,indianPerCap,AsianPerCap,OtherPerCap,HispPerCap,NumUnderPov,PctPopUnderPov,PctLess9thGrade,PctNotHSGrad,PctBSorMore,PctUnemployed,PctEmploy,PctEmplManu,PctEmplProfServ,PctOccupManu,PctOccupMgmtProf,MalePctDivorce,MalePctNevMarr,FemalePctDiv,TotalPctDiv,PersPerFam,PctFam2Par,PctKids2Par,PctYoungKids2Par,PctTeen2Par,PctWorkMomYoungKids,PctWorkMom,NumKidsBornNeverMar,PctKidsBornNeverMar,NumImmig,PctImmigRecent,PctImmigRec5,PctImmigRec8,PctImmigRec10,PctRecentImmig,PctRecImmig5,PctRecImmig8,PctRecImmig10,PctSpeakEnglOnly,PctNotSpeakEnglWell,PctLargHouseFam,PctLargHouseOccup,PersPerOccupHous,PersPerOwnOccHous,PersPerRentOccHous,PctPersOwnOccup,PctPersDenseHous,PctHousLess3BR,MedNumBR,HousVacant,PctHousOccup,PctHousOwnOcc,PctVacantBoarded,PctVacMore6Mos,MedYrHousBuilt,PctHousNoPhone,PctWOFullPlumb,OwnOccLowQuart,OwnOccMedVal,OwnOccHiQuart,OwnOccQrange,RentLowQ,RentMedian,RentHighQ,RentQrange,MedRent,MedRentPctHousInc,MedOwnCostPctInc,MedOwnCostPctIncNoMtg,NumInShelters,NumStreet,PctForeignBorn,PctBornSameState,PctSameHouse85,PctSameCity85,PctSameState85,LemasSwornFT,LemasSwFTPerPop,LemasSwFTFieldOps,LemasSwFTFieldPerPop,LemasTotalReq,LemasTotReqPerPop,PolicReqPerOffic,PolicPerPop,RacialMatchCommPol,PctPolicWhite,PctPolicBlack,PctPolicHisp,PctPolicAsian,PctPolicMinor,OfficAssgnDrugUnits,NumKindsDrugsSeiz,PolicAveOTWorked,LandArea,PopDens,PctUsePubTrans,PolicCars,PolicOperBudg,LemasPctPolicOnPatr,LemasGangUnitDeploy,LemasPctOfficDrugUn,PolicBudgPerPop,murders,murdPerPop,rapes,rapesPerPop,robberies,robbbPerPop,assaults,assaultPerPop,burglaries,burglPerPop,larcenies,larcPerPop,autoTheft,autoTheftPerPop,arsons,arsonsPerPop,ViolentCrimesPerPop,nonViolPerPop\n",
    "BerkeleyHeightstownship,NJ,39,5320,11980,3.1,1.37,91.78,6.5,1.88,12.47,21.44,10.93,11.33,11980,100,75122,89.24,1.55,70.2,23.62,1.03,18.39,79584,29711,30233,13600,5725,27101,5115,22838,227,1.96,5.81,9.9,48.18,2.7,64.55,14.65,28.82,5.49,50.73,3.67,26.38,5.22,4.47,3.22,91.43,90.17,95.78,95.81,44.56,58.88,31,0.36,1277,8.69,13,20.99,30.93,0.93,1.39,2.24,3.3,85.68,1.37,4.81,4.17,2.99,3,2.84,91.46,0.39,11.06,3,64,98.37,91.01,3.12,37.5,1959,0,0.28,215900,262600,326900,111000,685,1001,1001,316,1001,23.8,21.1,14,11,0,10.66,53.72,65.29,78.09,89.14,,,,,,,,,,,,,,,,,,6.5,1845.9,9.63,,,,,0,,0,0,0,0,1,8.2,4,32.81,14,114.85,138,1132.08,16,131.26,2,16.41,41.02,1394.59\n",
    "Marpletownship,PA,45,47616,23123,2.82,0.8,95.57,3.44,0.85,11.01,21.3,10.48,17.18,23123,100,47917,78.99,1.11,64.11,35.5,2.75,22.85,55323,20148,20191,18137,0,20074,5250,12222,885,3.98,5.61,13.72,29.89,2.43,61.96,12.26,29.28,6.39,37.64,4.23,27.99,6.45,5.42,3.11,86.91,85.33,96.82,86.46,51.14,62.43,43,0.24,1920,5.21,8.65,13.33,22.5,0.43,0.72,1.11,1.87,87.79,1.81,4.25,3.34,2.7,2.83,1.96,89.03,1.01,23.6,3,240,97.15,84.88,0,18.33,1958,0.31,0.14,136300,164200,199900,63600,467,560,672,205,627,27.6,20.7,12.5,0,0,8.3,77.17,71.27,90.22,96.12,,,,,,,,,,,,,,,,,,10.6,2186.7,3.84,,,,,0,,0,0,1,4.25,5,21.26,24,102.05,57,242.37,376,1598.78,26,110.55,1,4.25,127.56,1955.95"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy wiersz odpowiada jednej społeczności. Niektóre z cech, które można znaleźć w bazie to procentowy udział czarnoskórych, białych, azjatów i hiszpańskojęzycznych w społeczności, mediana dochodów i procent ludności żyjących poniżej granicy ubóstwa. Baza danych zawiera również informacje o przestępczości, takie jak liczba morderstw, gwałtów, napadów itp.\n",
    "\n",
    "Jak widać kolumn (cech społeczności) jest ponad 100 i jest to dosyć rozbudowana baza danych. W związku z tym, że jest to baza danych o przestępczości, to celem jest przewidzenie wskaźnika przestępczości na podstawie danych społeczności."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "\n",
    "Obie wersje preprocessingu które przeprowadziłem mają ze sobą to wspólnego, że wyrzucam z nich pierwsze 4 kolumny dotyczące nazwy społeczności, stanu w którym się znajduje itd., ponieważ nie jest to istotne w tym zadaniu. Następnie po wyliczeniu ogólnej przestępczości na 100000 mieszkańców pozbywam się także kolumn ze składnikami tej liczby. Wstawiam również kolumnę z zaklasyfikowaniem do jednego z 10 przedziałów ilości przestępstw na 100000 mieszkańców aby ułatwić algorytmom odgadywanie. W pierwszej wersji usuwam wszystkie kolumny w których NaN stanowi 50% a potem wszystkie pozostałe NaN zastępuję zerami. W drugiej wersji dokonuję większych i bardziej skomplikowanych zmian - zamiast po prostu usuwać kolumny, puste wartości w nich zastępuję za pomocą interpolate(). Następnie jeżeli zostały jakieś wartości NaN to zastępuję je zerami. W tym przypadku również korzystam ze skalowania i normalizacji danych (jest to już w pliku main2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('crimedata.csv', sep=',', header=0)\n",
    "data = data.drop(data.columns[[0, 1, 2, 3]], axis=1)\n",
    "threshold = len(data) * 0.5\n",
    "data = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "data = data.fillna(0)\n",
    "total_crimes_per_pop = (data[\"murders\"] + data[\"rapes\"] + data[\"robberies\"] + data[\"assaults\"] + data[\"burglaries\"] + data[\"larcenies\"] + data[\"autoTheft\"]) / data[\"population\"] * 100000\n",
    "data[\"totalCrimesPerPop\"] = total_crimes_per_pop\n",
    "bins = pd.cut(data['totalCrimesPerPop'], bins=10)\n",
    "data['crimeRates_categories'] = bins.astype(str)\n",
    "data = data.drop(data.columns[[102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120]], axis=1)\n",
    "data.to_csv('crimedatapreprocessed1.csv', sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('crimedata.csv', sep=',', header=0)\n",
    "data = data.drop(data.columns[[0, 1, 2, 3]], axis=1)\n",
    "for column in data.columns:\n",
    "    data[column] = data[column].interpolate()\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "total_crimes_per_pop = (data[\"murders\"] + data[\"rapes\"] + data[\"robberies\"] + data[\"assaults\"] + data[\"burglaries\"] + data[\"larcenies\"] + data[\"autoTheft\"]) / data[\"population\"] * 100000\n",
    "data[\"totalCrimesPerPop\"] = total_crimes_per_pop\n",
    "bins = pd.cut(data['totalCrimesPerPop'], bins=10)\n",
    "data['crimeRates_categories'] = bins.astype(str)\n",
    "data = data.drop(data.columns[[102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120]], axis=1)\n",
    "data.to_csv('crimedatapreprocessed2.csv', sep=',', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie w pliku main.py dokonuję podziału danych na zbiór treningowy i testowy, a następnie przeprowadzam uczenie i testowanie algorytmów. W tym przypadku korzystam z algorytmów: DecisionTreeClassifier (w dwóch wersjach), GaussianNB, KNeighborsClassifier (w trzech wersjach), MLPClassifier (również w trzech wersjach). W pliku main2.py dokonuję tego samego, ale z tym bardziej skomplikowanym i efektywnym preprocessingiem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"crimedatapreprocessed1.csv\")\n",
    "\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2137)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_all = label_encoder.fit_transform(np.concatenate([y_train, y_test]))\n",
    "y_train = y_all[:len(y_train)]\n",
    "y_test = y_all[len(y_train):]\n",
    "\n",
    "\n",
    "dtc_small = DecisionTreeClassifier(max_depth=5, random_state=2137)\n",
    "dtc_large = DecisionTreeClassifier(random_state=2137)\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn3 = KNeighborsClassifier(n_neighbors=30)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,7), max_iter=1000, random_state=2137)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=2137)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(10,7,5), max_iter=1000, random_state=2137)\n",
    "\n",
    "dtc_small.fit(X_train, y_train)\n",
    "dtc_large.fit(X_train, y_train)\n",
    "gnb.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "knn2.fit(X_train, y_train)\n",
    "knn3.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp2.fit(X_train, y_train)\n",
    "mlp3.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dtc_small = dtc_small.predict(X_test)\n",
    "y_pred_dtc_large = dtc_large.predict(X_test)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_pred_knn2 = knn2.predict(X_test)\n",
    "y_pred_knn3 = knn3.predict(X_test)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "y_pred_mlp2 = mlp2.predict(X_test)\n",
    "y_pred_mlp3 = mlp3.predict(X_test)\n",
    "\n",
    "cm_dtc_small = confusion_matrix(y_test, y_pred_dtc_small)\n",
    "cm_dtc_large = confusion_matrix(y_test, y_pred_dtc_large)\n",
    "cm_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "cm_knn2 = confusion_matrix(y_test, y_pred_knn2)\n",
    "cm_knn3 = confusion_matrix(y_test, y_pred_knn3)\n",
    "cm_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "cm_mlp2 = confusion_matrix(y_test, y_pred_mlp2)\n",
    "cm_mlp3 = confusion_matrix(y_test, y_pred_mlp3)\n",
    "\n",
    "print('Dokładność dtc_small: {:.2f}%'.format(accuracy_score(y_test, y_pred_dtc_small)*100))\n",
    "# print('Macierz błędu:\\n', cm_dtc_small)\n",
    "print('Dokładność dtc_large: {:.2f}%'.format(accuracy_score(y_test, y_pred_dtc_large)*100))\n",
    "# print('Macierz błędu:\\n', cm_dtc_large)\n",
    "print('Dokładność gnb: {:.2f}%'.format(accuracy_score(y_test, y_pred_gnb)*100))\n",
    "# print('Macierz błędu:\\n', cm_gnb)\n",
    "print('Dokładność knn: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn)\n",
    "print('Dokładność knn2: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn2)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn2)\n",
    "print('Dokładność knn3: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn3)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn3)\n",
    "print('Dokładność mlp: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp)\n",
    "print('Dokładność mlp2: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp2)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp2)\n",
    "print('Dokładność mlp3: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp3)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"crimedatapreprocessed2.csv\")\n",
    "\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2137)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_all = label_encoder.fit_transform(np.concatenate([y_train, y_test]))\n",
    "y_train = y_all[:len(y_train)]\n",
    "y_test = y_all[len(y_train):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "normalizer=MinMaxScaler()\n",
    "normalizer.fit(X_train)\n",
    "X_train=normalizer.transform(X_train)\n",
    "X_test=normalizer.transform(X_test)\n",
    "\n",
    "\n",
    "dtc_small = DecisionTreeClassifier(max_depth=5, random_state=2137)\n",
    "dtc_large = DecisionTreeClassifier(random_state=2137)\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn3 = KNeighborsClassifier(n_neighbors=30)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,7), max_iter=1000, random_state=2137)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=2137)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(10,7,5), max_iter=1000, random_state=2137)\n",
    "\n",
    "dtc_small.fit(X_train, y_train)\n",
    "dtc_large.fit(X_train, y_train)\n",
    "gnb.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "knn2.fit(X_train, y_train)\n",
    "knn3.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp2.fit(X_train, y_train)\n",
    "mlp3.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dtc_small = dtc_small.predict(X_test)\n",
    "y_pred_dtc_large = dtc_large.predict(X_test)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_pred_knn2 = knn2.predict(X_test)\n",
    "y_pred_knn3 = knn3.predict(X_test)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "y_pred_mlp2 = mlp2.predict(X_test)\n",
    "y_pred_mlp3 = mlp3.predict(X_test)\n",
    "\n",
    "cm_dtc_small = confusion_matrix(y_test, y_pred_dtc_small)\n",
    "cm_dtc_large = confusion_matrix(y_test, y_pred_dtc_large)\n",
    "cm_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "cm_knn2 = confusion_matrix(y_test, y_pred_knn2)\n",
    "cm_knn3 = confusion_matrix(y_test, y_pred_knn3)\n",
    "cm_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "cm_mlp2 = confusion_matrix(y_test, y_pred_mlp2)\n",
    "cm_mlp3 = confusion_matrix(y_test, y_pred_mlp3)\n",
    "\n",
    "print('Dokładność dtc_small: {:.2f}%'.format(accuracy_score(y_test, y_pred_dtc_small)*100))\n",
    "# print('Macierz błędu:\\n', cm_dtc_small)\n",
    "print('Dokładność dtc_large: {:.2f}%'.format(accuracy_score(y_test, y_pred_dtc_large)*100))\n",
    "# print('Macierz błędu:\\n', cm_dtc_large)\n",
    "print('Dokładność gnb: {:.2f}%'.format(accuracy_score(y_test, y_pred_gnb)*100))\n",
    "# print('Macierz błędu:\\n', cm_gnb)\n",
    "print('Dokładność knn: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn)\n",
    "print('Dokładność knn2: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn2)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn2)\n",
    "print('Dokładność knn3: {:.2f}%'.format(accuracy_score(y_test, y_pred_knn3)*100))\n",
    "# print('Macierz błędu:\\n', cm_knn3)\n",
    "print('Dokładność mlp: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp)\n",
    "print('Dokładność mlp2: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp2)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp2)\n",
    "print('Dokładność mlp3: {:.2f}%'.format(accuracy_score(y_test, y_pred_mlp3)*100))\n",
    "# print('Macierz błędu:\\n', cm_mlp3)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
